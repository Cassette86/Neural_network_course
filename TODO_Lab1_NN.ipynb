{"cells":[{"cell_type":"markdown","metadata":{"id":"657JLvobVUwO"},"source":["# Regression with DNN"]},{"cell_type":"markdown","metadata":{"id":"o5woqp05VuWR"},"source":["**Objectives :**\n","\n","* Predicts housing prices from a set of house features.\n","* Understanding the principle and the architecture of a regression with a dense neural network with backup and restore of the trained model.\n","\n","The [Boston Housing Prices Dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) consists of price of houses in various places in Boston.\n","Alongside with price, the dataset also provide these information :\n","\n","* CRIM: This is the per capita crime rate by town\n","* ZN: This is the proportion of residential land zoned for lots larger than 25,000 sq.ft\n","* INDUS: This is the proportion of non-retail business acres per town\n","* CHAS: This is the Charles River dummy variable (this is equal to 1 if tract bounds river; 0 otherwise)\n","* NOX: This is the nitric oxides concentration (parts per 10 million)\n","* RM: This is the average number of rooms per dwelling\n","* AGE: This is the proportion of owner-occupied units built prior to 1940\n","* DIS: This is the weighted distances to five Boston employment centers\n","* RAD: This is the index of accessibility to radial highways\n","* TAX: This is the full-value property-tax rate per 10,000 dollars\n","* PTRATIO: This is the pupil-teacher ratio by town\n","* B: This is calculated as 1000(Bk â€” 0.63)^2, where Bk is the proportion of people of African American descent by town\n","* LSTAT: This is the percentage lower status of the population\n","* MEDV: This is the median value of owner-occupied homes in 1000 dollars\n","\n","**What we're going to do :**\n","\n","* (Retrieve data)\n","* (Preparing the data)\n","* (Build a model)\n","* Train and save the model\n","* Restore saved model\n","* Evaluate the model\n","* Make some predictions\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3383,"status":"ok","timestamp":1699439258696,"user":{"displayName":"Fouad Hadj Selem","userId":"11326462517326991585"},"user_tz":-60},"id":"X_N8j7NlK95Q"},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From c:\\Users\\cassi\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n","\n"]}],"source":["# import os\n","# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import os,sys\n","\n","from IPython.display import Markdown\n","from importlib import reload\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JSBaR2oheVaV"},"outputs":[],"source":["#from google.colab import files\n","#uploaded = files.upload()"]},{"cell_type":"markdown","metadata":{"id":"6d8HIl4AM1Dx"},"source":["Load the boston housing dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3138,"status":"ok","timestamp":1687197614417,"user":{"displayName":"Walid Kheriji","userId":"17231722363260848260"},"user_tz":-120},"id":"KdjivSeQdX6M","outputId":"dcef007b-f727-4cbb-adc5-91128385a6ca"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["import pandas as pd\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","data = pd.read_csv('gdrive/My Drive/BostonHousing.csv', sep=',',header=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V_wB6gOtYZg5"},"outputs":[],"source":["\n","\n","display(data.head(5).style.format(\"{0:.2f}\"))\n","print('Missing Data : ',data.isna().sum().sum(), '  Shape is : ', data.shape)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uSJDYfo5fPHC"},"source":["## Exe 1: Data Preparation\n","\n","\n","Use 70% of the data for training and 30% for validation.\n","The dataset is shuffled and shared between learning and testing.\n","x will be input data and y the expected output\n","Check the data shape and sizes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w4qzQudGfPsH"},"outputs":[],"source":["# ---- Shuffle and Split => train, test\n","#\n","data       = data.sample(frac=1., axis=0)\n","data_train = data.sample(frac=0.7, axis=0)\n","data_test  = data.drop(data_train.index)\n","\n","# ---- Split => x,y (medv is price)\n","#\n","x_train = data_train.drop('medv',  axis=1)\n","y_train = data_train['medv']\n","x_test  = data_test.drop('medv',   axis=1)\n","y_test  = data_test['medv']\n","\n","print('Original data shape was : ',data.shape)\n","print('x_train : ',x_train.shape, 'y_train : ',y_train.shape)\n","print('x_test  : ',x_test.shape,  'y_test  : ',y_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"kTIW5DG1gH3s"},"source":["## Exe 2: Data normalization\n","\n","Normalizing data For each feature in the input data (a column in the\n","input data matrix), subtract the mean of the feature and divide by the standard\n","deviation\n","\n","**Note :**\n","\n","* All input data must be normalized, train and test.\n","* To do this we will subtract the mean and divide by the standard deviation.\n","* But test data should not be used in any way, even for normalization.\n","* The mean and the standard deviation will therefore only be calculated with the train data.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"irUk5OvCK9qm"},"outputs":[],"source":["display(x_train.describe().style.format(\"{0:.2f}\").set_caption(\"Before normalization :\"))\n","\n","mean = x_train.mean()\n","std  = x_train.std()\n","x_train = # TODO\n","x_test  = # TODO\n","\n","display(x_train.describe().style.format(\"{0:.2f}\").set_caption(\"After normalization :\"))\n","display(x_train.head(5).style.format(\"{0:.2f}\").set_caption(\"Few lines of the dataset :\"))\n","\n","x_train, y_train = # TODO\n","x_test,  y_test  = # TODO"]},{"cell_type":"markdown","metadata":{"id":"tk5PT-GONvp0"},"source":["## Exo4:  Model Design"]},{"cell_type":"markdown","metadata":{"id":"J_kotgwULVcH"},"source":["Design a neural network architecture with two hidden layers, each consisting of 64 units and utilizing the ReLU activation function. To obtain a linear output layer, include a final layer with a single unit and no activation function. Compile the network using the mean squared error (MSE) loss function. Additionally, monitor a new metric, mean absolute error (MAE), during training. Finally, define the optimizer as 'rmsprop' in the model compilation.\n","\n","Define the model within a function named build_model(shape)\n","\n","\n","More informations about :\n","\n","\n","* [Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)\n","* [Activation](https://www.tensorflow.org/api_docs/python/tf/keras/activations)\n","* [Loss](https://www.tensorflow.org/api_docs/python/tf/keras/losses)\n","* [Metrics](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wA391yVKNX3n"},"outputs":[],"source":["from keras import models\n","from keras import layers\n","def build_model (shape):\n","\n","  # TODO\n","\n","  return model\n","# complete the model here\n"]},{"cell_type":"markdown","metadata":{"id":"4iv88RDcQP7s"},"source":["## Exe 5 : Model Building\n","\n"," Use the build_model function to create a model with a shape equal to the number of features. Display its summary."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Djh2Efl8aDDH"},"outputs":[],"source":["\n","\n","model=build_model( (13,) )\n","\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"x6Px4Da0b1mx"},"source":["\n","\n","## Exe 6 : Model Fitting\n","\n","Fit the model on the train set with the test set as the\n","validation set, 60 epochs and save it in history.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_HcO_9Y5b2jP"},"outputs":[],"source":["history = # TODO"]},{"cell_type":"markdown","metadata":{"id":"YpxzW-1p2EXJ"},"source":["## Exe 7 : Model evaluation\n","\n","It is the moment for checking the model performance on the test dataset.\n","\n","Check the test loss and mae of the model.\n","\n","* MAE = Mean Absolute Error (between the labels and predictions)\n","\n","=> A mae equal to 3 represents an average error in prediction of $3k."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eBXglsV82RKG"},"outputs":[],"source":["\n","\n","score = # TODO\n","\n","print('x_test / loss      : {:5.4f}'.format(score[0]))\n","print('x_test / mae       : {:5.4f}'.format(score[1]))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"33BEc6MW4ojs"},"source":["## Exe 8 : Training histroy\n","\n","What was the best result during our training ?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k66U8rtf6GnC"},"outputs":[],"source":["df=pd.DataFrame(data=history.history)\n","display(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":404,"status":"ok","timestamp":1687200731976,"user":{"displayName":"Walid Kheriji","userId":"17231722363260848260"},"user_tz":-120},"id":"q3CC28B16UHT","outputId":"828a3a7f-520a-4bea-8dbc-6bb544a81c26"},"outputs":[{"name":"stdout","output_type":"stream","text":["min( val_mae ) : 2.3227\n"]}],"source":["print(\"min( val_mae ) : {:.4f}\".format( min(history.history[\"val_mae\"]) ) )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":575,"status":"ok","timestamp":1687200734865,"user":{"displayName":"Walid Kheriji","userId":"17231722363260848260"},"user_tz":-120},"id":"2Ps1brPN6xEh","outputId":"2c6bf989-0887-4bc4-9374-f18ce9fae8b0"},"outputs":[{"data":{"text/plain":["dict_keys(['loss', 'mae', 'val_loss', 'val_mae'])"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["history.history.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2V_QxA_-6XQ2"},"outputs":[],"source":["\n","plt.plot(history.history['loss'],linestyle='--', marker='+', label='train' )\n","plt.plot(history.history['val_loss'],linestyle='--', marker='o', label='test')\n","plt.title('LOSS', pad=-50)\n","plt.legend()\n","#plt.title('lrate='+str(lrate), pad=-50)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DPlUFKOy77Cr"},"outputs":[],"source":["plt.plot(history.history['mae'],linestyle='--', marker='+', label='train' )\n","plt.plot(history.history['val_mae'],linestyle='--', marker='o', label='test')\n","plt.title('MAE', pad=-50)\n","plt.legend()"]},{"cell_type":"markdown","metadata":{"id":"4tuYScCT8Rah"},"source":["## Exe 9: Model Prediction\n","\n","Use the trained neural network model, to predict the price of a given house and compare the result with the ground truth"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fiRxcz2m8Ybg"},"outputs":[],"source":["\n","\n","my_house = [ 1.26425925, -0.48522739,  1.0436489 , -0.23112788,  1.37120745,\n","       -2.14308942,  1.13489104, -1.06802005,  1.71189006,  1.57042287,\n","        0.77859951,  0.14769795,  2.7585581 ]\n","real_price = 10.4\n","\n","my_house= # TODO\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5REqbpBf8cF_"},"outputs":[],"source":["\n","\n","predictions = # TODO\n","print(\"Prediction : {:.2f} K$\".format(predictions[0][0]))\n","print(\"Reality    : {:.2f} K$\".format(real_price))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Al8LGaguGbFw"},"source":["## Exe 10 Early stoping\n","\n","Create a neural network with the following specifications:\n","\n","* Number of hidden layers: 2\n","* Number of units in each hidden layer: 64\n","* Activation function for the hidden layers: ReLU\n","* Output layer: Single unit with no activation function (for linear output)\n","* Loss function: Mean Squared Error (MSE)\n","* Additional metric to monitor during training: Mean Absolute Error (MAE)\n","* Optimizer: 'rmsprop'\n","* Apply dropout after each hidden layer with a dropout rate of 0.5 (50% of nodes dropped randomly for the following layer)\n","* Minibatch size: 100\n","* Use early stopping for fitting the model with patience=20 and epochs = 300\n","* Save the best model as 'model_best'\n","* Evaluate the saved model on the test data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VHeSqPpOHleE"},"outputs":[],"source":["  model = keras.models.Sequential()\n","  # TODO\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l6g9Bx-DGarX"},"outputs":[],"source":["from keras.callbacks import ModelCheckpoint\n","from keras.callbacks import EarlyStopping\n","from keras.models import load_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t4cpGLcuHN_y"},"outputs":[],"source":["es = EarlyStopping(# TODO)\n","mc = ModelCheckpoint(# TODO)\n","model.compile(# TODO)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lh0TNO83Hdzi"},"outputs":[],"source":["history = model.fit(# TODO)\n","saved_model = load_model('model_best.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k4Yn5vSPHjsK"},"outputs":[],"source":["test_loss, test_acc = # TODO\n","print(test_loss, test_acc)"]},{"cell_type":"markdown","metadata":{"id":"9ukmkExmVf2Y"},"source":["# Classification with DNN"]},{"cell_type":"markdown","metadata":{"id":"ki33nGO6PBw4"},"source":["## Exe 1:  Import data\n","\n","Import the cifra10 data set. The CIFAR-10 dataset consists of 60000\n","32 x 32 colour images in 10 classes, with 6000 images per class. There are\n","50000 training images and 10000 test images: https://www.tensorflow.org/datasets/catalog/cifar10"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15713,"status":"ok","timestamp":1687204837792,"user":{"displayName":"Walid Kheriji","userId":"17231722363260848260"},"user_tz":-120},"id":"TVAGkI_1PlxE","outputId":"a9a14d94-2649-4660-a68d-931b2d0f870c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170498071/170498071 [==============================] - 11s 0us/step\n"]}],"source":["data = keras . datasets . cifar10\n","cifar10_data = data . load_data ()"]},{"cell_type":"markdown","metadata":{"id":"2Fs6Kfq3PpAF"},"source":["## Exe 2 :\n","\n","Before using a dataset, the datatype should be checked. Test *type(cifar10 data)*\n","for verifying the variable type. *len(cifar10 data)* is another command for\n","checking the data size."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":389,"status":"ok","timestamp":1687205187192,"user":{"displayName":"Walid Kheriji","userId":"17231722363260848260"},"user_tz":-120},"id":"zU7mG6kZQ__a","outputId":"ebc115aa-8dde-48a3-f89c-3f63ce83447d"},"outputs":[{"data":{"text/plain":["tuple"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["type(cifar10_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":371,"status":"ok","timestamp":1687205190451,"user":{"displayName":"Walid Kheriji","userId":"17231722363260848260"},"user_tz":-120},"id":"rojeY8rwP82_","outputId":"815a9939-b1f6-44df-caf5-d1b9fec63774"},"outputs":[{"data":{"text/plain":["2"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["\n","len(cifar10_data)"]},{"cell_type":"markdown","metadata":{"id":"t5nwGMbpP9rF"},"source":["## Exe 3: Train and Test\n","\n","Load train and test images and labels with:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aHu03ewPTlHP"},"outputs":[],"source":["(train_images, train_labels),(test_images, test_labels) = cifar10_data"]},{"cell_type":"markdown","metadata":{"id":"Bs_zxmn8Tmml"},"source":["## Exe 4:\n","\n","The images are 32 x 32 NumPy arrays, with pixel values ranging from 0\n","to 255. You can check an example with:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EF2nXJGYTqlE"},"outputs":[],"source":["print ( train_images[0])\n","print ( train_images[0].shape )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tdJasNPVTv8G"},"outputs":[],"source":["np.unique(train_labels)"]},{"cell_type":"markdown","metadata":{"id":"Es46ai0xT26Q"},"source":["## Exe 5: Chek\n","\n","Check the dataset using the following commands:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MWaAn4KBT2C4"},"outputs":[],"source":["print(\"train_images shape : \",train_images.shape)\n","\n","print(\"train_labels size\", len(train_labels))\n","\n","print(\"train_labels\",train_labels)\n","\n","print(\"test_images shape : \",test_images.shape)\n","\n","print(\"test_labels\",test_labels)"]},{"cell_type":"markdown","metadata":{"id":"Q3rD-pstT_mN"},"source":["## Exe 6: Image Visualization\n","\n","An interesting fact about the image is that it can be plotted. To visualize an image, you can use the following code:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jAyspozuUD-I"},"outputs":[],"source":["index = 50\n","plt.figure()\n","plt.imshow(train_images[index])\n","plt.colorbar()\n","plt.show()\n","\n","train_labels[index]"]},{"cell_type":"markdown","metadata":{"id":"Hiaui9iOUJHi"},"source":["## Exe 7 : Display images and class name\n","\n","To verify that the data is in the correct format and that you're ready to\n","build and train the network, let's display the first 25 images from the training\n","set and display the class name below each image."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8HJTPi3vULkD"},"outputs":[],"source":["class_names = [ 'airplane' , 'automobile' , 'bird' , 'cat' , 'deer' , 'dog' , 'frog' , 'horse' , 'ship' , 'truck']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"05yOlft8UWBa"},"outputs":[],"source":["plt.figure(figsize=(20,20))\n","for i in range(25):\n","    # TODO\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"5TMDlNttUTu5"},"source":["## Exe 8: Dataset Normalization\n","\n","Normalize the train and test sets using the following code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fPI6ND4eUesV"},"outputs":[],"source":["train_images_before = train_images\n","test_images_before = test_images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H88JwUjfUg9S"},"outputs":[],"source":["train_images = # TODO\n","test_images = # TODO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v7JiMXwfUkES"},"outputs":[],"source":["train_images"]},{"cell_type":"markdown","metadata":{"id":"n3byMSmNUos5"},"source":["## Exo 9: Model\n","\n","Build a neural network with the following structure:\n","\n","\n","1.   The first layer should be a **Flatten** layer that transforms the format of the images from a two-dimensional array (32 by 32 pixels) to a one-dimensional array (1024 pixels). This layer simply unstacks the rows of pixels in the image and lines them up. It has no parameters to learn and only reformats the data.\n","\n","2.   After the pixels are flattened, the network should consist of two **Dense** layers. The first **Dense** layer should have 128 neurons.\n","\n","3.   The second (and last) layer should be a **softmax** layer with 10 neurons, which returns an array of 10 probability scores that sum to 1. Each neuron contains a score indicating the probability that the current image belongs to one of the 10 classes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4QuOBz6vUsvH"},"outputs":[],"source":["model = keras.Sequential([\n","    # TODO\n","])"]},{"cell_type":"markdown","metadata":{"id":"I6nx9weiU4Zp"},"source":["## Exe 10: Train the model\n","\n","Compile the model using the appropriate optimizer, loss function, and metrics. Set the optimizer to 'adam', the loss function to 'sparse_categorical_crossentropy', and the metric to 'accuracy'.\n","\n","Train the model, using the fit method on the model object. Pass in the train_images as the training data and train_labels as the corresponding labels."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qU6jh_NpU5T_"},"outputs":[],"source":["model.compile(# TODO)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f9jDzezRVCVv"},"outputs":[],"source":["model.fit(# TODO)"]},{"cell_type":"markdown","metadata":{"id":"IHZX17lKVTg6"},"source":["## Exe 11: Evaluate the model\n","\n","Chek the model performance on the test dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NJelBNIzVY1o"},"outputs":[],"source":["test_loss, test_acc = model.evaluate(# TODO)\n","\n","print('\\nTest accuracy:', test_acc)"]},{"cell_type":"markdown","metadata":{"id":"LbTek6WTVaNg"},"source":["## Exo12\n","\n","Now that the model has been trained, we can use it to make predictions on some images.\n","\n","1.  Use the trained model to make predictions on the test_images dataset by calling model.predict(test_images). This will generate predictions for each image in the testing set.\n","\n","2. Print the first, second, and third element of the predicted test set. Each element contains 10 values representing the probability of each label.\n","\n","3. After printing the predicted test set, use the np.argmax() function to choose the label with the highest probability for each element.\n","\n","4. Compare the predicted label for the first three elements with their actual labels. How many of them are correct?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o-ycSqmMVnci"},"outputs":[],"source":["predictions = # TODO\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bott6dprVqsL"},"outputs":[],"source":["# TODO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8KZ6ieyMVyyb"},"outputs":[],"source":["# TODO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mQPxi44EV1Ps"},"outputs":[],"source":["# TODO"]},{"cell_type":"markdown","metadata":{"id":"pGA_52v_V4ZL"},"source":["## Exe13 :  Checking Predicted Labels\n","\n","Now, let's write a function to check the predicted labels and visualize the results. The function should display the predicted labels similar to the above Figure, indicating the probability of the predicted label. If the prediction is correct, the label should be displayed in blue, otherwise in red."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BAb9tMhjV6IJ"},"outputs":[],"source":["def plot_image(i, predictions_array, true_label, img):\n","\n","  # TODO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mno_VQAoV-N7"},"outputs":[],"source":["class_names = [ 'airplane' , 'automobile' , 'bird' , 'cat' , 'deer' , 'dog' , 'frog' , 'horse' , 'ship' , 'truck']\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"33p62VQEWAf2"},"outputs":[],"source":["i = 50\n","plt.figure(figsize=(6,3))\n","plot_image(i, predictions[i], test_labels[i], test_images[i])\n","plt.show()\n","predictions[i]"]},{"cell_type":"markdown","metadata":{"id":"4ODJiuu3u4JE"},"source":["# Homework Assignment: Deep Neural Network Classification with Fashion MNIST\n","\n","Redo the classification part, this time utilizing the Fashion MNIST dataset and a DNN model with three Dense layers. The first and second Dense layers should have 128 neurons each. Additionally, incorporate the following specifications:\n","\n","* Apply dropout after each hidden layer with a dropout rate of 0.25 (25% of neurons dropped randomly for the subsequent layer).\n","*  Set the batch size to 512.\n","*  Implement early stopping while fitting the model with a patience value of 20 and a maximum number of epochs set to 200.\n","*  Save the best model as 'model_best'.\n","*  Evaluate the saved model on the test data.\n","\n","**For the submission of your work, please submit your completed assignment in HTML format.**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ajp5lR0OrInH"},"source":["# Converting a notebook to html"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3570,"status":"ok","timestamp":1687246722416,"user":{"displayName":"Walid Kheriji","userId":"17231722363260848260"},"user_tz":-120},"id":"a-0EJTVlrHYe","outputId":"0e741964-9c80-4cd1-99a4-ad1757f98b76"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","[NbConvertApp] Converting notebook /content/gdrive/MyDrive/Colab Notebooks/DL_Lab1_NN.ipynb to html\n","[NbConvertApp] Writing 652630 bytes to /content/gdrive/MyDrive/Colab Notebooks/DL_Lab1_NN.html\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","#%%shell\n","!jupyter nbconvert --to html '/content/gdrive/MyDrive/Colab Notebooks/DL_Lab1_NN.ipynb'"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1x4JHYPDIwK2eWMMDxR00-UYkDokkshnF","timestamp":1687211070013}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
